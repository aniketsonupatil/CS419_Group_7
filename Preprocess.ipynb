{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import pickle, time\n",
    "import re, os, string, typing, gc, json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    print(\"Length of data: \", len(data['data']))\n",
    "    print(\"Data Keys: \", data['data'][0].keys())\n",
    "    print(\"Title: \", data['data'][0]['title'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data:dict)->list:\n",
    "    data = data['data']\n",
    "    qa_list = []\n",
    "\n",
    "    for paragraphs in data:\n",
    "\n",
    "        for para in paragraphs['paragraphs']:\n",
    "            context = para['context']\n",
    "\n",
    "            for qa in para['qas']:\n",
    "                \n",
    "                id = qa['id']\n",
    "                question = qa['question']\n",
    "                if 'why' not in question and 'how' not in question: #need to remake df file\n",
    "                  for ans in qa['answers']:\n",
    "                      answer = ans['text']\n",
    "                      ans_start = ans['answer_start']\n",
    "                      ans_end = ans_start + len(answer)\n",
    "                      qa_dict = {}\n",
    "                      qa_dict['id'] = id\n",
    "                      qa_dict['context'] = context\n",
    "                      qa_dict['question'] = question\n",
    "                      qa_dict['label'] = [ans_start, ans_end]\n",
    "\n",
    "                      qa_dict['answer'] = answer\n",
    "                      qa_list.append(qa_dict)    \n",
    "\n",
    "    \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Beyonc√©\n"
     ]
    }
   ],
   "source": [
    "train_file = os.path.join( './','train-v2.0.json')\n",
    "train_data = load_json(train_file)\n",
    "train_list = parse_data(train_data)\n",
    "train_df = pd.DataFrame(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  35\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Normans\n"
     ]
    }
   ],
   "source": [
    "dev_file = os.path.join( './','dev-v2.0.json')\n",
    "dev_data = load_json(dev_file)\n",
    "dev_list = parse_data(dev_data)\n",
    "dev_df = pd.DataFrame(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    def to_lower(text):\n",
    "        return text.lower()\n",
    "    df.context = df.context.apply(to_lower)\n",
    "    df.question = df.question.apply(to_lower)\n",
    "    df.answer = df.answer.apply(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df(train_df)\n",
    "preprocess_df(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84548\n",
      "19880\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(dev_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7976\\3026600295.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  total_df = train_df.append(dev_df)\n"
     ]
    }
   ],
   "source": [
    "total_df = train_df.append(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104428"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_for_vocab(df):    \n",
    "    text = []\n",
    "    total = 0\n",
    "    unique_contexts = list(df.context.unique())\n",
    "    unique_questions = list(df.question.unique())\n",
    "    total += df.context.nunique() + df.question.nunique()\n",
    "    text.extend(unique_contexts + unique_questions)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text =text_for_vocab(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabbuilder(vocab_text):\n",
    "    words = []\n",
    "    for sent in vocab_text:\n",
    "        for word in nlp(sent, disable=['parser','tagger','ner']):\n",
    "            words.append(word.text)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    #word_vocab = list(set(word_vocab).intersection(set(glove_words)))\n",
    "    word_vocab.insert(0, '<unk>')\n",
    "    word_vocab.insert(1, '<pad>')\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    return word2idx, idx2word, word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 94198\n",
      "vocab-length: 94200\n",
      "word2idx-length: 94200\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, word_vocab = vocabbuilder(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_ids(text, word2idx):\n",
    "    #converts words to numerical index in the text\n",
    "    context_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\n",
    "    context_ids = [word2idx[word] for word in context_tokens]\n",
    "    \n",
    "    assert len(context_ids) == len(context_tokens)\n",
    "    return context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_ids(text, word2idx):\n",
    "    question_tokens = [w.text for w in nlp(text, disable=['parser','tagger','ner'])]\n",
    "    question_ids = [word2idx[word] for word in question_tokens]\n",
    "    \n",
    "    assert len(question_ids) == len(question_tokens)\n",
    "    return question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(df, idx2word):\n",
    "    '''\n",
    "    Performs the tests mentioned above. This method also gets the start and end of the answers\n",
    "    with respect to the context_ids for each example.\n",
    "    \n",
    "    :param dataframe df: SQUAD df\n",
    "    :returns\n",
    "        list start_value_error: example idx where the start idx is not found in the start spans\n",
    "                                of the text\n",
    "        list end_value_error: example idx where the end idx is not found in the end spans\n",
    "                              of the text\n",
    "        list assert_error: examples that fail assertion errors. A majority are due to the above errors\n",
    "        \n",
    "    '''\n",
    "\n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser','tagger','ner'])]\n",
    "       \n",
    "        try:\n",
    "          start_token = answer_tokens[0]\n",
    "          end_token = answer_tokens[-1]\n",
    "          #print(start_token,end_token)\n",
    "        except:\n",
    "          assert_error.append(index)\n",
    "          #print(index)\n",
    "          #print(answer_tokens)\n",
    "          continue\n",
    "\n",
    "        \n",
    "        context_span  = [(word.idx, word.idx + len(word.text)) \n",
    "                         for word in nlp(row['context'], disable=['parser','tagger','ner'])]\n",
    "        #print(row['context'])\n",
    "\n",
    "        starts, ends = zip(*context_span)\n",
    "        \n",
    "\n",
    "        answer_start, answer_end = row['label']\n",
    "        #print(starts)\n",
    "\n",
    "        try:\n",
    "            start_idx = starts.index(answer_start)\n",
    "            \n",
    "        except:\n",
    "            start_value_error.append(index)\n",
    "            #print('start err')\n",
    "            #print(index)\n",
    "            #print(answer_tokens)\n",
    "            #print(starts)\n",
    "            #print(ends)\n",
    "            #print(answer_start)\n",
    "            \n",
    "        try:\n",
    "            end_idx  = ends.index(answer_end)\n",
    "            \n",
    "        except:\n",
    "            end_value_error.append(index)\n",
    "            #print(\"enderror\")\n",
    "            #print(index)\n",
    "            #print(answer_tokens)\n",
    "            #print(starts,ends)\n",
    "            #print(answer_end)\n",
    "            \n",
    "\n",
    "        try:\n",
    "            \n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\n",
    "        except:\n",
    "            assert_error.append(index)\n",
    "            #print(\"index err\")\n",
    "            #print(index)\n",
    "            \n",
    "        # print(\"TESTING.......\")\n",
    "        # print(idx2word[row['context_ids'][start_idx]])\n",
    "        # print(idx2word[row['context_ids'][end_idx]])\n",
    "        # print(answer_tokens)\n",
    "        # print(answer_tokens[0])\n",
    "        # print(answer_tokens[-1])  \n",
    "            #print(starts,ends)\n",
    "                   \n",
    "\n",
    "    \n",
    "    return start_value_error, end_value_error, assert_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_indices(df, idx2word):\n",
    "    '''\n",
    "    Gets error indices from the method above and returns a \n",
    "    set of those indices.\n",
    "    '''\n",
    "    \n",
    "    start_value_error, end_value_error, assert_error = test_indices(df,idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f\"Error indices: {len(err_idx)}\")\n",
    "    \n",
    "    return err_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>beyonc√© giselle knowlescarter biÀêÀàj…ínse…™ beeyo...</td>\n",
       "      <td>when did beyonce start becoming popular?</td>\n",
       "      <td>[269, 286]</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>[1117, 40900, 56297, 56298, 56299, 838, 429, 8...</td>\n",
       "      <td>[30, 20, 1741, 448, 1109, 269, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "\n",
       "                                             context  \\\n",
       "0  beyonc√© giselle knowlescarter biÀêÀàj…ínse…™ beeyo...   \n",
       "\n",
       "                                   question       label             answer  \\\n",
       "0  when did beyonce start becoming popular?  [269, 286]  in the late 1990s   \n",
       "\n",
       "                                         context_ids  \\\n",
       "0  [1117, 40900, 56297, 56298, 56299, 838, 429, 8...   \n",
       "\n",
       "                        question_ids  \n",
       "0  [30, 20, 1741, 448, 1109, 269, 6]  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error indices: 879\n"
     ]
    }
   ],
   "source": [
    "train_err = get_error_indices(train_df, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(train_err, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_answer(row, idx2word):\n",
    "    '''\n",
    "    Takes in a row of the dataframe or one training example and\n",
    "    returns a tuple of start and end positions of answer by calculating \n",
    "    spans.\n",
    "    ''' \n",
    "    context_span = [(word.idx, word.idx + len(word.text)) for word in nlp(row.context, disable=['parser','tagger','ner'])]\n",
    "    starts, ends = zip(*context_span)\n",
    "    \n",
    "    answer_start, answer_end = row.label\n",
    "    start_idx = starts.index(answer_start)\n",
    " \n",
    "    end_idx  = ends.index(answer_end)\n",
    "    \n",
    "    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','tagger','ner'])]\n",
    "    ans_start = ans_toks[0]\n",
    "    ans_end = ans_toks[-1]  \n",
    "    assert idx2word[row.context_ids[start_idx]] == ans_start\n",
    "    assert idx2word[row.context_ids[end_idx]] == ans_end\n",
    "    \n",
    "    return [start_idx, end_idx]  #this needs to be end_idx +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label_idx'] = train_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>when did beyonce start becoming popular?</td>\n",
       "      <td>[269, 286]</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>[929, 39213, 17687, 15, 10123, 22, 52767, 1217...</td>\n",
       "      <td>[36, 26, 1447, 471, 1175, 289, 7]</td>\n",
       "      <td>[56, 59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>what areas did beyonce compete in when she was...</td>\n",
       "      <td>[207, 226]</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>[929, 39213, 17687, 15, 10123, 22, 52767, 1217...</td>\n",
       "      <td>[11, 221, 26, 1447, 3152, 6, 36, 326, 13, 1178...</td>\n",
       "      <td>[44, 46]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>when did beyonce leave destiny's child and bec...</td>\n",
       "      <td>[526, 530]</td>\n",
       "      <td>2003</td>\n",
       "      <td>[929, 39213, 17687, 15, 10123, 22, 52767, 1217...</td>\n",
       "      <td>[36, 26, 1447, 1701, 4665, 18, 663, 8, 184, 10...</td>\n",
       "      <td>[112, 112]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>in what city and state did beyonce  grow up?</td>\n",
       "      <td>[166, 180]</td>\n",
       "      <td>houston, texas</td>\n",
       "      <td>[929, 39213, 17687, 15, 10123, 22, 52767, 1217...</td>\n",
       "      <td>[6, 11, 53, 8, 76, 26, 1447, 296, 1947, 106, 7]</td>\n",
       "      <td>[36, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>in which decade did beyonce become famous?</td>\n",
       "      <td>[276, 286]</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>[929, 39213, 17687, 15, 10123, 22, 52767, 1217...</td>\n",
       "      <td>[6, 28, 1115, 26, 1447, 184, 601, 7]</td>\n",
       "      <td>[58, 59]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                             context  \\\n",
       "0  beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...   \n",
       "1  beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...   \n",
       "2  beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...   \n",
       "3  beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...   \n",
       "4  beyonc√© giselle knowles-carter (/biÀêÀàj…ínse…™/ b...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0           when did beyonce start becoming popular?  [269, 286]   \n",
       "1  what areas did beyonce compete in when she was...  [207, 226]   \n",
       "2  when did beyonce leave destiny's child and bec...  [526, 530]   \n",
       "3      in what city and state did beyonce  grow up?   [166, 180]   \n",
       "4         in which decade did beyonce become famous?  [276, 286]   \n",
       "\n",
       "                answer                                        context_ids  \\\n",
       "0    in the late 1990s  [929, 39213, 17687, 15, 10123, 22, 52767, 1217...   \n",
       "1  singing and dancing  [929, 39213, 17687, 15, 10123, 22, 52767, 1217...   \n",
       "2                 2003  [929, 39213, 17687, 15, 10123, 22, 52767, 1217...   \n",
       "3       houston, texas  [929, 39213, 17687, 15, 10123, 22, 52767, 1217...   \n",
       "4           late 1990s  [929, 39213, 17687, 15, 10123, 22, 52767, 1217...   \n",
       "\n",
       "                                        question_ids   label_idx  \n",
       "0                  [36, 26, 1447, 471, 1175, 289, 7]    [56, 59]  \n",
       "1  [11, 221, 26, 1447, 3152, 6, 36, 326, 13, 1178...    [44, 46]  \n",
       "2  [36, 26, 1447, 1701, 4665, 18, 663, 8, 184, 10...  [112, 112]  \n",
       "3    [6, 11, 53, 8, 76, 26, 1447, 296, 1947, 106, 7]    [36, 38]  \n",
       "4               [6, 28, 1115, 26, 1447, 184, 601, 7]    [58, 59]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_glove_dict():\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "\n",
    "    '''\n",
    "    glove_dict = {}\n",
    "    with open(os.path.join( './','glove.6B.100d.txt'), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            glove_dict[word] = vector\n",
    "    glove_dict[\"<NULL>\"] = np.asarray([0. for _ in range(100)])\n",
    "    glove_dict[\"<unk>\"] = np.asarray([0. for _ in range(100)])\n",
    "            \n",
    "    f.close()   \n",
    "    return glove_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = make_glove_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights_matrix(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 100))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[word2idx[word]] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return weights_matrix, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in the GloVe vocab:  71069\n"
     ]
    }
   ],
   "source": [
    "weights_matrix, words_found = create_weights_matrix(glove_dict)\n",
    "print(\"Words found in the GloVe vocab: \" ,words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94200"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"]\", \" ] \")\n",
    "    text = text.replace(\"[\", \" [ \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('./data','word_vocab.pkl'), \"wb\") as wv, \\\n",
    "      open(os.path.join('./data','word2index.pkl'), \"wb\") as wi, \\\n",
    "         open(os.path.join('./data','index2word.pkl'), \"wb\") as iw:\n",
    "         pickle.dump(word_vocab, wv)\n",
    "         pickle.dump(word2idx, wi)\n",
    "         pickle.dump(idx2word,iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('traindata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('weights_matrix_100D.npy', weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('./data','vocab_text.pkl'), \"wb\") as vt:\n",
    "    pickle.dump(vocab_text, vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
